About the database and the website
----------------------------------

We created the IPSL (Iconic Patterns in Sign Languages) database and
website to study iconicity patterns in sign languages from a typological
perspective and using quantitative methods.

The Data
--------

In order to create the IPSL database, we used the on-line dictionary of
sign languages [Spreadthesign](www.spreadthesign.com). This dictionary
contains video recordings of isolated signs and signed sentences (up to
15 000 entries per language) in 31 sign languages. The dictionary has
been created as a tool to facilitate learning of sign languages across
the world; it has not been specifically designed for linguistic
research. For instance, typically only one sign per concept is provided,
while a sign language might have several signs. In addition, the
procedure of data collection lead to the fact that lexical signs and
multi-sign descriptions of non-lexicalized concepts are not
systematically distinguished. However, since we focused only on the most
basic concrete concepts, we consider the data to be good enough for our
purposes.

We selected nineteen sign languages from all languages present at the
website, namely Russian, French, American, British , Spanish, Italian,
German, Polish, Brazilian, Turkish, Portuguese, Czech, Lithuanian,
Swedish, Greek, Romanian, Latvian, Estonian, and Icelandic Sign
Languages. The choice was governed by two main considerations: (1) we
only selected the languages for which a majority of the 15 000 signs
were present on the website at the moment of selection (the fall of
2016); (2) we excluded Ukranian and Belorussian Sign Language due to
their close relatedness to Russian Sign Language.

We included 87 concrete concepts from seven semantic fields: transport,
nature, instruments (tools), house, clothes, food, and animals. We
reasoned that this selection represents a reasonable sample of the basic
concrete concepts. It turned out that some sign languages are missing
signs for some of the concepts. Furthermore, we decided to exclude any
entries where a concept was described by a sequence of more than two
signs, as they are unlikely to be lexicalized. This resulted in the
database of 1542 annotated signs (out of the 1653 theoretically possible
items).

Annotating the data
-------------------

In this study, we re-annotated the whole data-set that was first
annotated by Klezovich (2017). Klezovich’s annotation was based on
Taub’s (2001; 2012) approach to iconicity in sign languages. We modified
and extended Klezovich’s approach in order to make the annotation
procedure more clear and reliable, and also more complete.

### Previous approaches to iconicity

Taub (2001) offers a structural approach to iconicity. According to her,
creation of iconic signs involves several steps: image selection (a
particular image associated with the concept to be encoded is selected),
schematization (simplification of the image), and encoding (selecting
language-specific linguistic means to create a sign resembling the
schematized image). In her (2012) chapter, she discusses how image
selection and encoding can be further classified.

Specifically, image selection (aka concept-image association) can
involve the following mechanisms:

1.  Parts stand for wholes. For instance, the concept ‘cat’ is
    associated with one notable part of the cat, namely the whiskers,
    and this image is then used in the iconic sign, as in the RSL sign
    [CAT](https://media.spreadthesign.com/video/mp4/12/13448.mp4).
2.  Prototypical member. For instance, the concept ‘tree’ is associated
    with a prototypical example of a tree, namely a tree with an
    extended trunk and a large round-shaped system of branches, as in
    the RSL sign
    [TREE](https://media.spreadthesign.com/video/mp4/12/39508.mp4).
3.  Body movement or action associated with the concept. For instance,
    the concept ‘car’ is associated with an image of holding and turning
    the wheel, as in the RSL sign
    [CAR](https://media.spreadthesign.com/video/mp4/12/345841.mp4).
4.  Associated concept. For some (usually abstract) concepts, another
    concept is selected which is clearly associated with it. For
    instance, the concept ‘Olympics’ is associated with the
    linked-circle logo, so this latter concept can be selected for
    further encoding, as in the ASL sign
    [OLYMPIC](https://media.spreadthesign.com/video/mp4/13/308156.mp4).

Taub further discusses encoding (aka form-image association). According
to her, encoding can be globally classified into full-size mapping (the
whole body of the signer, including hands, arms, the head, and the upper
body, is mapped onto the body of a person or an animal) and hand-size
mapping (only the hand is mapped to some features of the image, while
the rest of the body is not), which is further classified into three
subtypes. The whole classification is as follows:

1.  Full-size mapping. For instance, the image of hammering a nail is
    mapped to the signer performing a hammering activity while holding a
    hammer, as in the RSL sign
    [HAMMER](https://media.spreadthesign.com/video/mp4/12/56.mp4).
2.  Hand-size mapping.

-   Object mapping. The hand represents the object itself, as in the RSL
    sign [CHAIR](https://media.spreadthesign.com/video/mp4/12/7905.mp4).
-   Contour mapping. The hand (usually two hands) represents the outline
    or surface of an object. For instance, the two hands touching at an
    angle can represent the roof of a house in the RSL sign \[HOUSE\]
    (<https://media.spreadthesign.com/video/mp4/12/349172.mp4>).
-   Tracing mapping. The hands move to trace an outline or surface of
    an object. For instance, a flat hand can move in an arc-shaped
    trajectory to outline the surface of a mountain, as in the RSL sign
    [MOUNTAIN](https://media.spreadthesign.com/video/mp4/12/16407.mp4).

Klezovich (2017) used these two classifications (concept-image
association and form-image association) to annotate the iconic signs in
her sample with small modifications. She uses a different terminology,
but the underlying concepts are very similar if not identical.
Specifically, she uses the terms “handling” for “full-size mapping”,
“form similarity” for “prototypical member”, “property-holder” for
“associated concept”, “iconicity pattern” for “form-image association”
and “iconicity base” for “concept-image association”. She also discusses
in some detail the problems with these classifications, especially with
the concept-image associations. We turn to these problems and to our
modifications intended to address them in the following section.

### Featural annotation of iconicity

The classification of form-meaning associations is in principle
consistent and easy to apply. One difficulty concerns the exact status
of the body-size mapping. Quite often in the literature, the term
“handling” is used to describe this type of association, and Klezovich
(2017) used the same term as well. However, sings where the whole body
of the signer is mapped to another person or animal do not necessarily
depict handling of objects. Specifically, in many signs for animals, the
hands are mapped to paws or wings of the animal, and the upper body and
head to the corresponding parts of the animal, but no handling is
involved (e.g. the Polish SL sign
[BIRD](https://media.spreadthesign.com/video/mp4/19/59866.mp4)).
Therefore, using the term “handling” for whole-body mappings is not
always correct, but simply using the term “body-size mapping” does not
distinguish between the handling and non-handling cases.

In order to address this issue, we decided to separate this
classification into two separate classifications: form-image mapping and
personification. Personification is a binary feature, and we define it
as follows: the sign is \[+personification\] if the whole body (the
hands, arms, upper body, and head) are a part of the iconic
representation. The form-image mapping in our definition can contain one
of four values: handling, object, contour, or tracing. Not all
combinations of the two features are possible: if the sign has contour
or tracing mapping, it cannot be \[+personification\], as the hands do
not represent the hands of a person or animal. However, both handling
and object mappings are compatible with personification, and the
handling mapping always implies personification. The relation between
the two variables is represented in the table:

<table>
<thead>
<tr class="header">
<th></th>
<th>  </th>
<th>+ personification</th>
<th>  </th>
<th>- personification</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Handling</td>
<td></td>
<td>e.g. RSL <a href="https://media.spreadthesign.com/video/mp4/12/56.mp4">HAMMER</a></td>
<td></td>
<td>does not exist</td>
</tr>
<tr class="even">
<td>Object</td>
<td></td>
<td>e.g. PSL <a href="https://media.spreadthesign.com/video/mp4/19/59866.mp4">BIRD</a></td>
<td></td>
<td>e.g. RSL <a href="https://media.spreadthesign.com/video/mp4/12/7905.mp4">CHAIR</a></td>
</tr>
<tr class="odd">
<td>Contour</td>
<td></td>
<td>does not exist</td>
<td></td>
<td>e.g. RSL <a href="https://media.spreadthesign.com/video/mp4/12/349172.mp4">HOUSE</a></td>
</tr>
<tr class="even">
<td>Tracing</td>
<td></td>
<td>does not exist</td>
<td></td>
<td>e.g. RSL <a href="https://media.spreadthesign.com/video/mp4/12/16407.mp4">MOUNTAIN</a></td>
</tr>
</tbody>
</table>

--

The main body of problems concerns the concept-image associations. As
Klezovich discusses, unlike the form-image associations, concept-image
associations are not mutually exclusive. For instance, the prototypical
member association appears to be a default: it is always present. For
instance, if the sign describes a part of an animal (e.g. whiskers of a
cat) and thus represents a part-whole association, it often if not
always describes a prototypical part of a prototypical animal. The same
is true for the associated action association: if the sign describes
some action associated with an object (e.g. turning a key, see RSL
[KEY](https://media.spreadthesign.com/video/mp4/12/228353.mp4)), it is a
prototypical action and a prototypical object in most if not all cases.
The part-whole and the associated action are also compatible with each
other: a sign for elephant often refers to the trunk of an elephant, but
it also can move in a manner resembling the movement of an elephant’s
trunk (e.g. Brazilian SL sign
[ELEPHANT](https://media.spreadthesign.com/video/mp4/14/314556.mp4)).
Finally, the associated concept association normally concerns abstract
signs only, so it might have to be considered separately, alongside with
metaphorical associations.

Furthermore, there seems to be a set of intricate relations between
form-image associations and concept-image associations. For instance,
the tracing form-image association is incompatible with associated
action concept-image association, while the other three types of
form-image association are compatible with it. On the other hand, the
handling form-image association seems incompatible with the part-whole
association.

In order to address these problems, we again decided to decompose the
concept-image association into several features. Thus, we introduce the
binary “associated action” feature which is annotated as positive if the
movement of the sign depicts the movement of the object that it refers
to. We also introduce the binary “part-whole” feature which is annotated
as positive if the sign depicts only a salient part of an object. We
decided to completely abandon the two other associations. The
prototypical member association is not necessary because it is always or
nearly always present. The associated concept association is only
necessary for abstract concepts, while our database only contains signs
for concrete concepts.

With the feature decomposition, the problem of mutual compatibility of
some associations is solved. However, this approach does not address the
incompatibility of some form-meaning associations with these features.
Thus we had to stipulate some rules to describe these incompatibilities.
In particular, we formulated the following rules in advance:

1.  The \[+personification\] feature necessarily implies \[-part-whole\]
2.  The \[tracing\] form-image mapping necessarily implies \[-associated
    action\]

The first rule is based on the idea that if the whole body is involved
in representing a person or an animal \[+personification\], the sign
simply cannot represent only a part of it. The second rule is based on
the idea that in the \[tracing\] signs the movement is used to outline
an object, so it cannot also reflect the movement of the object.

Finally, neither Taub’s (2001) nor Klezovich’s (2017) approach addresses
the following issue: quite often, signs have iconic locations. The
approaches described above primarily concern iconic use of handshape and
movement. Specifically, in all types of form-image mappings, the
handshape is used iconically; if the signs is \[+associated movement\]
or \[+tracing\], the movement is necessarily iconic as well. However,
whether the location of the sign is iconic or not, is not reflected by
any of the features. Moreover, as we discuss in the next section, even
signs which would be classified as non-iconic, might have iconic
locations.

We thus introduced a new feature, “location”, which receives a positive
value if the location of the sign is iconic, that is, it represents the
marked location of the object that is being represented. This happens
with signs like sun, moon, sky (see the RSL sign
[SUN](https://media.spreadthesign.com/video/mp4/12/17058.mp4)) which are
often located above the neutral space, or with pieces of clothing which
are often located on the body (see the RSL sign
[T-SHIRT](https://media.spreadthesign.com/video/mp4/12/14733.mp4)). We
also decided to formulate the following rule: neutral location is always
non-iconic. While this is not always true, it is extremely difficult to
prove that the neutral location is used iconically, as it is the default
location for most signs. To sum up, we classify the signs according to
the following features:

1.  Form-image association: handling/object/contour/tracing
2.  Personification: +/-
3.  Associated action: +/-
4.  Parts for wholes: +/-
5.  Location: +/-

And we use the following rules to automatically exclude some
combinations of features:

1.  \[tracing\] → \[-personification\]
2.  \[contour\] → \[-personification\]
3.  \[handling\] → \[+personification\]
4.  \[+personification\] → \[-part-whole\]
5.  \[tracing\] → \[-associated action\]
6.  Neutral location → \[-location\]

### Non-iconic signs

Following Klezovich, we distinguish two types of non-iconic signs: those
involving fingerspelling, and those which are simply non-iconic. The
signs of the former type are clearly non-iconic, but they can contain an
iconic location. For instance, the Spanish sign
[SUN](https://media.spreadthesign.com/video/mp4/5/17051.mp4) is a
fingerspelling sequence of the letters of the word *sol* ‘sun’, but the
hand producing the letters is located above the head of the signer, so
the location is clearly iconic.

The second type of non-iconic signs includes all signs for which we
could not come up with an iconic motivation. Note that this means that
the signs are in fact not transparent, but not necessarily non-iconic.
We also tried to be careful in assigning iconic motivation to dubious
cases, which means that in cases of doubt we would use the non-iconic
label instead of classifying the sign according to the other parameters.

To sum up, for non-iconic signs, we use the feature “non-iconic”, which
has the following values: fingerspelling/non-iconic. For iconic signs,
this feature is left empty. The other features on non-iconic signs
(discussed in the previous section) are filled in as “-”.

### Two-handed signs and compounds

Some signs have a complex morphological (or at least phonological)
structure. We distinguish two cases that are annotated following
additional rules, namely compounds and asymmetric two-handed signs.

Compounds are signs that have two clearly distinguishable sequential
parts. In practice, any sign with two different movements are analyzed
as compounds, even if the handshape and location are identical in the
two parts of the sign. For compounds, we annotate each part separately,
using the “&” sign to separate the values of features per part of the
sign: e.g. handling&object for a compound with the handling association
in the first part, and the object association in the second part, as in
the Italian SL sign
[SPOON](https://media.spreadthesign.com/video/mp4/17/178120.mp4).

We decided to include only the compounds consisting of two parts. It is
clear that some signs on [Spreadthesign](www.spreadthesign.com) are not
lexical items but phrasal or clausal descriptions of objects for which
no lexical items exist (e.g. see the following video for
[WARDROBE](https://media.spreadthesign.com/video/mp4/4/1847.mp4) in
Czech SL, literally BOX DOOR CLOTHES THROW). As it is not possible for
such cases to determine whether they are compounds or not, we apply the
simple heuristic of only including signs with two parts in our data set.
We thus probably overlook some real lexicalized compounds with three or
four parts, but this can only concern a small minority of cases.

Another case of complex signs are two-handed signs. In symmetric
two-handed signs, the two hands have the same handshape and movement, so
the hands are also necessarily the same with respect to the iconicity
features, and we do not annotate them separately. However, in asymmetric
two-handed signs the two hands might (and often do) depict separate
parts or aspects of the depicted object. For instance, in the sign
HELICOPTER, one hand represents the cabin and the other hand the turning
blades (e.g. in RSL:
[HELICOPTER](https://media.spreadthesign.com/video/mp4/12/34232.mp4)).
Since these two-parts can be different with respect to iconicity
features, we annotate them separately, using the “+” sign between the
parts. The active hand is always annotated first: e.g. handling+object,
see for instance the Turkish SL sign
[SPOON](https://media.spreadthesign.com/video/mp4/11/11821.mp4).

Sometimes one or both parts of a compound are asymmetric two-handed
signs. This represents the most complex type of cases in our dataset,
and it is annotated following the rules described above. For instance,
we annotate the form-image mapping of the sign Icelandic sign
[LAMP](https://media.spreadthesign.com/video/mp4/21/235285.mp4) as
tracing+contour&object+contour.

Two-handed signs present an additional complication for the
\[part-whole\] feature. There are two classes of cases for which the
second hand can be annotated as positive or negative with respect to
this feature, so we have to stipulate rules to annotate such cases
systematically.

One such case involves asymmetric signs where the active hand moves away
from the passive hand to trace the outline of the object, (e.g. in the
Icelandic SL sign
[RIVER](https://media.spreadthesign.com/video/mp4/21/54680.mp4)). It is
clear that the active hand represents the whole referent, but the
passive hand can be interpreted as representing some part of the
referent. However, since it is not clear what this part is, and whether
this is a salient part of the whole object, we gloss the passive hand in
all such cases as \[-part-whole\].

The second case concerns signs where the passive hand in fact represents
an object which is different from the main referent of the sign, but
which is used in the same situation. For instance, in the Turkish SL
sign [SPOON](https://media.spreadthesign.com/video/mp4/11/11821.mp4) the
passive hand represents a plate. This other referent can be interpreted
as being a part of the whole image, but, on the other hand, this
referent is not a part of the main referent of the sign. We thus also
always annotated the passive hand in such cases as \[-part-whole\].

### Reliability of annotations

In order to ascertain the reliability of the annotation procedure, the
whole data set (1542 sign tokens) has been fully annotated by Kimmelman
and Klezovich. We then compared the annotations to calculate agreement
per each of the features that we annotated.

In order to adjust the agreement estimation for chance agreement, we
calculated Cohen’s kappa statistics using the fmsb package (Nakazawa
2017) in R (R Core Team 2016). We considered compounds and asymmetric
two-handed signs as containing multiple observations, so we separated
them into separate cells (e.g. handling+object was turned into two
observations: handling and object, located in separate cells in the
dataset).

The resulting estimated values of Cohen’s kappa, with confidence
intervals and qualitative characterizations (as provided by the fmsb
package) for iconic features, are summarized in the table:

<table>
<thead>
<tr class="header">
<th>Feature</th>
<th>  </th>
<th>Cohen’s kappa</th>
<th>  </th>
<th>95% confidence interval</th>
<th>  </th>
<th>Characterization</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Form-image mapping</td>
<td></td>
<td>0.87</td>
<td></td>
<td>0.85-0.89</td>
<td></td>
<td>Almost perfect agreement</td>
</tr>
<tr class="even">
<td>Personification</td>
<td></td>
<td>0.78</td>
<td></td>
<td>0.75-0.81</td>
<td></td>
<td>Substantial agreement</td>
</tr>
<tr class="odd">
<td>Associated action</td>
<td></td>
<td>0.8</td>
<td></td>
<td>0.78-0.83</td>
<td></td>
<td>Almost perfect agreement</td>
</tr>
<tr class="even">
<td>Part-whole</td>
<td></td>
<td>0.65</td>
<td></td>
<td>0.61-0.69</td>
<td></td>
<td>Substantial agreement</td>
</tr>
<tr class="odd">
<td>Location</td>
<td></td>
<td>0.76</td>
<td></td>
<td>0.73-0.78</td>
<td></td>
<td>Substantial agreement</td>
</tr>
</tbody>
</table>

--

From the table it should be clear that we have high agreement for all
features, especially for the form-image mapping. The lowest agreement is
observed for the part-whole feature. This is explained by the fact that
the authors originally interpreted this feature in a different way for
three classes of signs. Firstly, in the signs for animals where the
hands represent paws/wings of an animal. According to the annotation
guidelines discussed above, such cases should be analyzed as
\[-part-whole\], as the whole body represents the body of the animal.
One of the authors however consistently glossed these signs as
\[+part-whole\]. The other two classes are the asymmetrical signs
discussed in the previous section, in which the role of the second hand
is unclear with respect to this feature. One of the authors annotated in
accordance with the rules formulated above, while the other did not.

After the quantitative analysis of agreement, the two authors discussed
the cases of disagreement and agreed upon a correct annotation for each
of those cases. Therefore, the final dataset can be considered reliably
annotated following the guidelines described in this study.

The IPSL website
----------------

To make the database easily accessible to other researchers interested
in iconicity, we created the IPSL website, built with *shiny* (Chang et
al., 2017). The website contains the full database (which can be
searched on-line with *dt* (Xie, 2016) or downloaded), and visualistion
tools.

The first tool visualizes the concepts on the map of the world. The user
can select one of the 84 concepts, which are repseresented as dots on
the map with color-coding of the form-image mapping. Each dot on the map
is clickable and opens the video of the relevant sign (see Figure 1).
The user can also filter the data points by specifying the values of the
associated action, location, personification, and part-whole
features. The maps are created with the *lingtypology* package (Moroz,
2017).

The second tool visualizes the semantic fields on the map. The user can
select one of the seven semantic fields, and then specify the form-image
mapping and the values of the associated action, location,
personification, and part-whole features; the signs which conform the
selection appear on the map. In addition, a table is generated
containing the concepts and languages with the relevant feature
combinations.

Finally, the third tool creates graphs (bar charts) built with ggplot2
(Wickham and Chang, 2016). The user can select a semantic field (or all
semantic fields), and the charts show the distribution of concepts by
the form-image mapping feature for each language, either in absolute
values or in percentages.

Note that all tools treat parts of compounds and the two hands in asymmetric two-handed signs as independent measurements, so they should only be used for initial inspection of the data. For careful analysis, please donwload the original data. 

Linguistic research
-------------------

We have conducted research using the IPSL database and website. The
results of this research are to be reported here in near future. If you
are interested in learning more about it, please contact us using the
e-mails on the home page.

References
----------

-   Chang, W., Cheng, J., Allaire, J., Xie, Y., and McPherson, J., 2017.
    *shiny: Web Application Framework for R.*
    <https://cran.r-project.org/package=shiny>.
-   Klezovich, A. 2017. Typology of Iconicity Patterns in
    Sign Languages. Term paper. Moscow, ms.
-   Moroz, G. 2017. *lingtypology: easy mapping for
    Linguistic Typology.*
    <https://cran.r-project.org/package=lingtypology>.
-   Nakazawa, M. 2017. *fmsb: Functions for Medical Statistics Book with
    some Demographic Data*. <https://cran.r-project.org/package=fmsb>.
-   R Core Team. 2016. *R: A Language and Environment for Statistical
    Computing*. Vienna, Austria: R Foundation for Statistical Computing.
    <https://www.R-project.org/>.
-   Taub, S.F. 2001. *Language from the body: iconicity and metaphor in
    American Sign Language*. Cambridge: Cambridge University Press.
-   Taub, S.F. 2012. Iconicity and metaphor. In R. Pfau, M. Steinbach
    & B. Woll (eds.), *Sign language: An international
    handbook*, 388–412. Berlin: De Gruyter Mouton.
-   Wickham, H. and Chang, W. 2016. *ggplot2: Create Elegant Data
    Visualisations Using the Grammar of Graphics.*
    <https://cran.r-project.org/package=ggplot2>.
-   Xie, Y. 2016. *DT: A Wrapper of the JavaScript
    Library \`DataTables’.* <https://cran.r-project.org/package=dt>.
